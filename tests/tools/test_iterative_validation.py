# Clear Thinking FastMCP Server - Iterative Validation Cycle Tests

"""
Comprehensive test suite for Iterative Validation Cycle cognitive tool.

This test suite validates the hypothesis-test-learn-refine cycle through:
- Hypothesis formation and validation
- Test design and execution
- Results analysis and learning extraction
- Refinement generation
- Cycle management and progression
"""

import pytest
import pytest_asyncio
from unittest.mock import AsyncMock, MagicMock
from typing import List, Dict, Any

# Import test models directly without Pydantic validators for testing
from dataclasses import dataclass
from enum import Enum


class ValidationStatus(str, Enum):
    NOT_STARTED = "not_started"
    HYPOTHESIS_FORMED = "hypothesis_formed"
    TEST_DESIGNED = "test_designed"
    TEST_IN_PROGRESS = "test_in_progress"
    RESULTS_COLLECTED = "results_collected"
    ANALYSIS_COMPLETE = "analysis_complete"
    LEARNINGS_EXTRACTED = "learnings_extracted"
    REFINED = "refined"


class ValidationTestType(str, Enum):
    EXPERIMENT = "experiment"
    PROTOTYPE = "prototype"
    SURVEY = "survey"
    A_B_TEST = "a_b_test"
    SIMULATION = "simulation"
    PILOT = "pilot"
    ANALYSIS = "analysis"
    USER_TEST = "user_test"
    TECHNICAL_TEST = "technical_test"
    MARKET_TEST = "market_test"


class ConfidenceLevel(str, Enum):
    VERY_LOW = "very_low"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    VERY_HIGH = "very_high"


class LearningType(str, Enum):
    CONFIRMATION = "confirmation"
    REFUTATION = "refutation"
    PARTIAL = "partial"
    UNEXPECTED = "unexpected"
    INCONCLUSIVE = "inconclusive"
    DIRECTIONAL = "directional"


@dataclass
class MockHypothesis:
    statement: str
    assumptions: List[str]
    success_criteria: List[str]
    confidence_level: ConfidenceLevel
    rationale: str
    risks: List[str] = None
    related_hypotheses: List[str] = None
    
    def __post_init__(self):
        if self.risks is None:
            self.risks = []
        if self.related_hypotheses is None:
            self.related_hypotheses = []


@dataclass
class MockTestDesign:
    test_type: ValidationTestType
    methodology: str
    metrics: List[str]
    sample_size: str = None
    duration: str = None
    resources_needed: List[str] = None
    controls: List[str] = None
    success_threshold: str = None
    
    def __post_init__(self):
        if self.resources_needed is None:
            self.resources_needed = []
        if self.controls is None:
            self.controls = []


@dataclass
class MockTestResults:
    raw_data: Dict[str, Any]
    key_findings: List[str]
    metrics_achieved: Dict[str, str]
    confidence_in_results: ConfidenceLevel
    unexpected_observations: List[str] = None
    limitations: List[str] = None
    
    def __post_init__(self):
        if self.unexpected_observations is None:
            self.unexpected_observations = []
        if self.limitations is None:
            self.limitations = []


@dataclass
class MockLearning:
    learning_type: LearningType
    key_insight: str
    supporting_evidence: List[str]
    implications: List[str]
    confidence_level: ConfidenceLevel
    actionable_items: List[str] = None
    
    def __post_init__(self):
        if self.actionable_items is None:
            self.actionable_items = []


@dataclass
class MockRefinement:
    original_element: str
    refinement_description: str
    rationale: str
    expected_improvement: str
    implementation_steps: List[str]


@dataclass
class MockValidationCycle:
    cycle_number: int
    hypothesis: MockHypothesis
    test_design: MockTestDesign
    test_results: MockTestResults
    learnings: List[MockLearning]
    refinements: List[MockRefinement]
    status: ValidationStatus
    duration: str = None


@dataclass
class MockIterativeValidationInput:
    scenario: str
    complexity_level: str
    session_id: str
    domain_context: str = None
    initial_hypothesis: MockHypothesis = None
    validation_constraints: Dict[str, str] = None
    previous_cycles: List[MockValidationCycle] = None
    target_confidence: ConfidenceLevel = ConfidenceLevel.HIGH
    max_iterations: int = None
    test_preferences: List[ValidationTestType] = None


# Mock context for testing
class MockContext:
    def __init__(self):
        self.progress_calls = []
        self.info_calls = []
        self.debug_calls = []
        self.error_calls = []
    
    async def progress(self, message: str, progress: float = 0.0):
        self.progress_calls.append((message, progress))
    
    async def info(self, message: str):
        self.info_calls.append(message)
    
    async def debug(self, message: str):
        self.debug_calls.append(message)
    
    async def error(self, message: str):
        self.error_calls.append(message)
    
    async def cancelled(self) -> bool:
        return False


class TestIterativeValidationLogic:
    """Test iterative validation cycle logic without full model dependencies"""
    
    def test_hypothesis_formation(self):
        """Test formation of testable hypotheses"""
        hypothesis = MockHypothesis(
            statement="Users will pay $99/month for premium analytics features",
            assumptions=[
                "Users value advanced analytics",
                "Price aligns with perceived value",
                "Features differentiate from competitors"
            ],
            success_criteria=[
                "50% conversion rate from trial",
                "Less than 5% monthly churn",
                "Positive ROI within 6 months"
            ],
            confidence_level=ConfidenceLevel.MEDIUM,
            rationale="Based on competitor analysis and initial user interviews",
            risks=["Price point too high", "Features not distinctive enough"]
        )
        
        assert hypothesis.confidence_level == ConfidenceLevel.MEDIUM
        assert len(hypothesis.assumptions) >= 3
        assert len(hypothesis.success_criteria) >= 3
        assert any("conversion" in criterion.lower() for criterion in hypothesis.success_criteria)
    
    def test_test_design_generation(self):
        """Test generation of appropriate test designs"""
        test_design = MockTestDesign(
            test_type=ValidationTestType.A_B_TEST,
            methodology="Split users 50/50 between $99 and $79 price points",
            metrics=[
                "Conversion rate from trial",
                "Monthly recurring revenue",
                "Churn rate",
                "Customer lifetime value"
            ],
            sample_size="200 trial users minimum",
            duration="3 months",
            resources_needed=["Engineering time for implementation", "Analytics setup"],
            controls=["User demographics", "Feature usage patterns"],
            success_threshold="Statistical significance at 95% confidence"
        )
        
        assert test_design.test_type == ValidationTestType.A_B_TEST
        assert len(test_design.metrics) >= 4
        assert test_design.sample_size is not None
        assert "significance" in test_design.success_threshold.lower()
    
    def test_results_analysis(self):
        """Test analysis of test results"""
        test_results = MockTestResults(
            raw_data={
                "conversion_rate_99": 0.42,
                "conversion_rate_79": 0.58,
                "churn_rate_99": 0.08,
                "churn_rate_79": 0.06,
                "sample_size": 214
            },
            key_findings=[
                "$79 price point shows 38% higher conversion",
                "Churn rates comparable between price points",
                "Total revenue higher at $79 due to volume"
            ],
            metrics_achieved={
                "Conversion Rate $99": "42%",
                "Conversion Rate $79": "58%",
                "Statistical Significance": "Yes (p=0.02)"
            },
            confidence_in_results=ConfidenceLevel.HIGH,
            unexpected_observations=["Enterprise users preferred $99 option"],
            limitations=["Limited to 3-month observation period"]
        )
        
        assert test_results.confidence_in_results == ConfidenceLevel.HIGH
        assert len(test_results.key_findings) >= 3
        assert test_results.raw_data["conversion_rate_79"] > test_results.raw_data["conversion_rate_99"]
    
    def test_learning_extraction(self):
        """Test extraction of learnings from results"""
        learning = MockLearning(
            learning_type=LearningType.PARTIAL,
            key_insight="Price sensitivity varies significantly by user segment",
            supporting_evidence=[
                "Individual users prefer $79 (65% conversion)",
                "Enterprise users prefer $99 (78% conversion)",
                "Overall revenue maximized at $79"
            ],
            implications=[
                "Consider segment-based pricing",
                "Enterprise features may justify premium pricing",
                "Volume at lower price point drives growth"
            ],
            confidence_level=ConfidenceLevel.HIGH,
            actionable_items=[
                "Implement tiered pricing model",
                "Develop enterprise-specific features",
                "Test $89 price point for optimization"
            ]
        )
        
        assert learning.learning_type == LearningType.PARTIAL
        assert len(learning.supporting_evidence) >= 3
        assert len(learning.actionable_items) >= 2
        assert "segment" in learning.key_insight.lower()
    
    def test_refinement_generation(self):
        """Test generation of refinements based on learnings"""
        refinement = MockRefinement(
            original_element="Single price point strategy",
            refinement_description="Implement tiered pricing with Individual ($79) and Enterprise ($99) plans",
            rationale="Test results show different price sensitivity by segment",
            expected_improvement="20% increase in total revenue through better segmentation",
            implementation_steps=[
                "Define clear feature differentiation",
                "Update billing system for tiers",
                "Create targeted marketing messages",
                "Train sales team on positioning"
            ]
        )
        
        assert len(refinement.implementation_steps) >= 4
        assert "tier" in refinement.refinement_description.lower()
        assert "revenue" in refinement.expected_improvement.lower()
    
    def test_validation_cycle_progression(self):
        """Test progression through validation cycles"""
        cycles = []
        
        # Cycle 1: Initial hypothesis
        cycle1 = MockValidationCycle(
            cycle_number=1,
            hypothesis=MockHypothesis(
                statement="Users will pay $99/month",
                assumptions=["Users value analytics"],
                success_criteria=["50% conversion rate"],
                confidence_level=ConfidenceLevel.MEDIUM,
                rationale="Competitor analysis"
            ),
            test_design=MockTestDesign(
                test_type=ValidationTestType.A_B_TEST,
                methodology="A/B test pricing",
                metrics=["Conversion rate"]
            ),
            test_results=MockTestResults(
                raw_data={"conversion": 0.42},
                key_findings=["Below target conversion"],
                metrics_achieved={"Conversion": "42%"},
                confidence_in_results=ConfidenceLevel.HIGH
            ),
            learnings=[
                MockLearning(
                    learning_type=LearningType.REFUTATION,
                    key_insight="$99 price point too high for most users",
                    supporting_evidence=["42% conversion vs 50% target"],
                    implications=["Need to test lower price points"],
                    confidence_level=ConfidenceLevel.HIGH
                )
            ],
            refinements=[
                MockRefinement(
                    original_element="$99 price point",
                    refinement_description="Test $79 price point",
                    rationale="Original price too high",
                    expected_improvement="Higher conversion rate",
                    implementation_steps=["Update pricing"]
                )
            ],
            status=ValidationStatus.REFINED,
            duration="3 months"
        )
        cycles.append(cycle1)
        
        # Cycle 2: Refined hypothesis
        cycle2 = MockValidationCycle(
            cycle_number=2,
            hypothesis=MockHypothesis(
                statement="Tiered pricing will maximize revenue",
                assumptions=["Different segments have different willingness to pay"],
                success_criteria=["60% overall conversion", "20% revenue increase"],
                confidence_level=ConfidenceLevel.HIGH,
                rationale="Learning from cycle 1 about segment differences"
            ),
            test_design=MockTestDesign(
                test_type=ValidationTestType.PILOT,
                methodology="Launch tiered pricing to subset",
                metrics=["Conversion by tier", "Total revenue"]
            ),
            test_results=MockTestResults(
                raw_data={"conversion_individual": 0.65, "conversion_enterprise": 0.78},
                key_findings=["Tiered model successful"],
                metrics_achieved={"Overall Conversion": "68%", "Revenue Increase": "24%"},
                confidence_in_results=ConfidenceLevel.VERY_HIGH
            ),
            learnings=[
                MockLearning(
                    learning_type=LearningType.CONFIRMATION,
                    key_insight="Segment-based pricing optimizes revenue",
                    supporting_evidence=["68% conversion", "24% revenue increase"],
                    implications=["Roll out to all users"],
                    confidence_level=ConfidenceLevel.VERY_HIGH
                )
            ],
            refinements=[],
            status=ValidationStatus.REFINED,
            duration="2 months"
        )
        cycles.append(cycle2)
        
        assert len(cycles) == 2
        assert cycles[0].cycle_number < cycles[1].cycle_number
        # HIGH confidence level is greater than MEDIUM
        assert cycles[0].hypothesis.confidence_level == ConfidenceLevel.MEDIUM
        assert cycles[1].hypothesis.confidence_level == ConfidenceLevel.HIGH
    
    def test_confidence_progression_tracking(self):
        """Test tracking of confidence progression across cycles"""
        confidence_progression = {
            1: ConfidenceLevel.MEDIUM,
            2: ConfidenceLevel.HIGH,
            3: ConfidenceLevel.VERY_HIGH
        }
        
        # Check progression
        confidence_values = list(confidence_progression.values())
        assert confidence_values[0] == ConfidenceLevel.MEDIUM
        assert confidence_values[-1] == ConfidenceLevel.VERY_HIGH
        
        # Verify improvement
        confidence_map = {
            ConfidenceLevel.VERY_LOW: 1,
            ConfidenceLevel.LOW: 2,
            ConfidenceLevel.MEDIUM: 3,
            ConfidenceLevel.HIGH: 4,
            ConfidenceLevel.VERY_HIGH: 5
        }
        
        values = [confidence_map[c] for c in confidence_values]
        assert all(values[i] <= values[i+1] for i in range(len(values)-1))
    
    @pytest.mark.asyncio
    async def test_domain_adaptation_product_development(self):
        """Test adaptation to product development domain"""
        mock_context = MockContext()
        
        input_data = MockIterativeValidationInput(
            scenario="Determining optimal feature set for mobile app",
            complexity_level="complex",
            session_id="test_product_1",
            domain_context="product_development",
            initial_hypothesis=MockHypothesis(
                statement="Users want all desktop features in mobile app",
                assumptions=["Feature parity is important", "Users use mobile as primary"],
                success_criteria=["High feature usage", "5-star app rating"],
                confidence_level=ConfidenceLevel.LOW,
                rationale="Initial assumption without user research"
            ),
            validation_constraints={
                "timeline": "6 months",
                "budget": "$50,000",
                "team_size": "3 developers"
            },
            max_iterations=4,
            test_preferences=[ValidationTestType.PROTOTYPE, ValidationTestType.USER_TEST]
        )
        
        assert input_data.domain_context == "product_development"
        assert input_data.initial_hypothesis.confidence_level == ConfidenceLevel.LOW
        assert ValidationTestType.PROTOTYPE in input_data.test_preferences
    
    @pytest.mark.asyncio
    async def test_domain_adaptation_research(self):
        """Test adaptation to research domain"""
        mock_context = MockContext()
        
        input_data = MockIterativeValidationInput(
            scenario="Testing new algorithm performance",
            complexity_level="complex",
            session_id="test_research_1",
            domain_context="research",
            initial_hypothesis=MockHypothesis(
                statement="New algorithm reduces processing time by 50%",
                assumptions=["Current bottlenecks identified correctly", "Algorithm scales linearly"],
                success_criteria=["50% time reduction", "Maintains accuracy above 95%"],
                confidence_level=ConfidenceLevel.MEDIUM,
                rationale="Theoretical analysis suggests improvement"
            ),
            test_preferences=[ValidationTestType.EXPERIMENT, ValidationTestType.SIMULATION, ValidationTestType.ANALYSIS]
        )
        
        assert input_data.domain_context == "research"
        assert ValidationTestType.EXPERIMENT in input_data.test_preferences
        assert ValidationTestType.SIMULATION in input_data.test_preferences
    
    def test_convergence_analysis(self):
        """Test analysis of convergence toward solution"""
        convergence_metrics = {
            "hypothesis_stability": 0.8,  # How much hypotheses change between cycles
            "confidence_growth": 0.9,     # Rate of confidence increase
            "learning_rate": 0.7,         # New insights per cycle
            "refinement_effectiveness": 0.85  # Success of refinements
        }
        
        overall_convergence = sum(convergence_metrics.values()) / len(convergence_metrics)
        assert overall_convergence > 0.7  # Good convergence
        assert convergence_metrics["confidence_growth"] > 0.8
    
    def test_pivot_detection(self):
        """Test detection of major pivots in validation journey"""
        pivots = [
            "Shifted from single pricing to tiered model",
            "Changed target from individual to enterprise focus",
            "Moved from feature parity to mobile-first design",
            "Pivoted from automation to augmentation approach"
        ]
        
        assert len(pivots) >= 3
        assert any("pricing" in pivot.lower() for pivot in pivots)
        assert any("focus" in pivot.lower() or "target" in pivot.lower() for pivot in pivots)
    
    def test_uncertainty_tracking(self):
        """Test tracking of remaining uncertainties"""
        remaining_uncertainties = [
            "Long-term retention at new price point",
            "Competitive response to pricing change",
            "International market price sensitivity",
            "Feature adoption rates over time"
        ]
        
        assert len(remaining_uncertainties) >= 3
        assert any("long" in u.lower() or "time" in u.lower() for u in remaining_uncertainties)
    
    def test_methodology_insights(self):
        """Test extraction of insights about validation process"""
        methodology_insights = [
            "A/B tests most effective for pricing decisions",
            "3-month cycles optimal for meaningful data",
            "User segmentation critical for accurate testing",
            "Qualitative feedback essential alongside metrics",
            "Early prototypes save development time"
        ]
        
        assert len(methodology_insights) >= 5
        assert any("segment" in insight.lower() for insight in methodology_insights)
        assert any("qualitative" in insight.lower() or "feedback" in insight.lower() for insight in methodology_insights)
    
    def test_failure_point_identification(self):
        """Test identification of validation failure points"""
        failure_points = [
            "Initial hypothesis too broad and untestable",
            "Sample size too small in first test",
            "Didn't account for seasonal variations",
            "Overlooked enterprise segment initially"
        ]
        
        assert len(failure_points) >= 3
        assert any("sample" in point.lower() or "size" in point.lower() for point in failure_points)
    
    @pytest.mark.asyncio
    async def test_full_iterative_validation_workflow(self):
        """Test complete iterative validation workflow"""
        mock_context = MockContext()
        
        # Step 1: Setup input
        input_data = MockIterativeValidationInput(
            scenario="Optimizing user onboarding flow for SaaS product",
            complexity_level="complex",
            session_id="integration_test_1",
            domain_context="product_development",
            initial_hypothesis=MockHypothesis(
                statement="Shorter onboarding (5 steps vs 10) increases completion rate",
                assumptions=[
                    "Users prefer quick setup",
                    "Essential info can be collected in 5 steps",
                    "Optional steps can be deferred"
                ],
                success_criteria=[
                    "80% completion rate",
                    "No increase in support tickets",
                    "Same activation rate"
                ],
                confidence_level=ConfidenceLevel.MEDIUM,
                rationale="Industry best practices suggest shorter is better"
            ),
            validation_constraints={
                "timeline": "2 months",
                "budget": "$10,000",
                "users": "500 new signups"
            },
            target_confidence=ConfidenceLevel.HIGH,
            max_iterations=3
        )
        
        # Step 2: Run first cycle
        cycle1 = MockValidationCycle(
            cycle_number=1,
            hypothesis=input_data.initial_hypothesis,
            test_design=MockTestDesign(
                test_type=ValidationTestType.A_B_TEST,
                methodology="Split test: 5-step vs 10-step onboarding",
                metrics=["Completion rate", "Time to complete", "Support tickets"],
                sample_size="250 users per variant",
                duration="3 weeks"
            ),
            test_results=MockTestResults(
                raw_data={
                    "completion_5_step": 0.72,
                    "completion_10_step": 0.68,
                    "support_tickets_increase": 0.15
                },
                key_findings=[
                    "5-step shows slight improvement",
                    "Support tickets increased 15%",
                    "Users confused about missing options"
                ],
                metrics_achieved={
                    "5-step completion": "72%",
                    "10-step completion": "68%",
                    "Support increase": "15%"
                },
                confidence_in_results=ConfidenceLevel.HIGH
            ),
            learnings=[
                MockLearning(
                    learning_type=LearningType.PARTIAL,
                    key_insight="Shorter flow improves completion but creates confusion",
                    supporting_evidence=[
                        "4% improvement in completion",
                        "15% increase in support burden",
                        "User feedback mentions missing options"
                    ],
                    implications=["Need better progressive disclosure"],
                    confidence_level=ConfidenceLevel.HIGH,
                    actionable_items=["Add optional steps after completion"]
                )
            ],
            refinements=[
                MockRefinement(
                    original_element="5-step linear flow",
                    refinement_description="5-step core + optional expansion",
                    rationale="Balance completion with functionality",
                    expected_improvement="80% completion with reduced support",
                    implementation_steps=[
                        "Design expandable sections",
                        "Add 'Setup Later' options",
                        "Create post-onboarding prompts"
                    ]
                )
            ],
            status=ValidationStatus.REFINED
        )
        
        # Step 3: Run second cycle with refinement
        cycle2 = MockValidationCycle(
            cycle_number=2,
            hypothesis=MockHypothesis(
                statement="Progressive onboarding (5 core + optional) optimizes completion and satisfaction",
                assumptions=["Users want control", "Core features sufficient for start"],
                success_criteria=["80% completion", "No support increase", "High satisfaction"],
                confidence_level=ConfidenceLevel.HIGH,
                rationale="Learning from cycle 1 about balance"
            ),
            test_design=MockTestDesign(
                test_type=ValidationTestType.PROTOTYPE,
                methodology="Test progressive onboarding with expandable options",
                metrics=["Completion rate", "Support tickets", "User satisfaction"],
                duration="4 weeks"
            ),
            test_results=MockTestResults(
                raw_data={
                    "completion_rate": 0.83,
                    "support_tickets": 0.98,  # Relative to baseline
                    "satisfaction_score": 4.3
                },
                key_findings=[
                    "83% completion rate achieved",
                    "Support tickets back to normal",
                    "Users appreciate flexibility"
                ],
                metrics_achieved={
                    "Completion": "83%",
                    "Support": "Normal levels",
                    "Satisfaction": "4.3/5"
                },
                confidence_in_results=ConfidenceLevel.VERY_HIGH
            ),
            learnings=[
                MockLearning(
                    learning_type=LearningType.CONFIRMATION,
                    key_insight="Progressive disclosure optimal for onboarding",
                    supporting_evidence=[
                        "Exceeded 80% target",
                        "Eliminated support burden",
                        "High user satisfaction"
                    ],
                    implications=["Roll out to all users", "Apply pattern elsewhere"],
                    confidence_level=ConfidenceLevel.VERY_HIGH
                )
            ],
            refinements=[],
            status=ValidationStatus.REFINED
        )
        
        # Step 4: Validate results
        assert cycle1.cycle_number < cycle2.cycle_number
        # Verify confidence progression
        assert cycle1.hypothesis.confidence_level == ConfidenceLevel.MEDIUM
        assert cycle2.hypothesis.confidence_level == ConfidenceLevel.HIGH
        assert cycle2.test_results.raw_data["completion_rate"] > 0.8
        
        # Step 5: Check convergence
        confidence_progression = {
            1: ConfidenceLevel.MEDIUM,
            2: ConfidenceLevel.VERY_HIGH
        }
        
        assert confidence_progression[2] == ConfidenceLevel.VERY_HIGH
        
        # Step 6: Context tracking
        await mock_context.info("Iterative validation completed successfully")
        await mock_context.progress("Validation complete", 1.0)
        
        assert len(mock_context.info_calls) > 0
        assert len(mock_context.progress_calls) > 0
        
        print(f"âœ… Full iterative validation workflow test passed")
        print(f"   Cycles completed: 2")
        print(f"   Final completion rate: 83%")
        print(f"   Confidence achieved: {cycle2.hypothesis.confidence_level}")
        print(f"   Key learning: Progressive disclosure optimal")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])